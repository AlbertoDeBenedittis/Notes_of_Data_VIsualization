{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a21f5a62",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7669b9",
   "metadata": {},
   "source": [
    "### Curse of dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915a2229",
   "metadata": {},
   "source": [
    "From adaptive control processes of Bellman: In absence of simplyfing assumptions, the sample size required to estimate a function of several variables to a given degree of accuracy (i.e. to get a reasonably low variance estimate) grows exponentially with the increasing number of variables.\n",
    "\n",
    "-> THE EMPTY SPACE PHENOMENON: high dimensional spaces are inherently sparse.\n",
    "\n",
    "Considering a sphere and a cube it is interesting to notice that for increasing D, the volume of the cube concentrates more in its cornerns and less in the inscribed sphere. \n",
    "\n",
    "Another example is the (hyper)-spherical shells. Virtually, al the content of a D dimensional sphere concentrates on its surface, which is only a (D-1) dimensional manifold. \n",
    "\n",
    "Normal distributions, the probability mass of a multivariate gaussian rapidly migrates into the trails, as the dimension increases.\n",
    "\n",
    "Dealing with high dimension can thus intruce high fluctaution. \n",
    "\n",
    "CONCENTRATION OF NORMS:\n",
    "\n",
    "Let $x = [x_{1}, ..., x_{D}] \\in R^D $ be a random vector whose components $x_{k}, 1 <= k <= D$, are independent and identically distributed with a finite eight order moment. Let $\\mu = E{x_{k}}$ be the common mean of all components $x_{k}$, and $\\mu_{j} = E((x_{k}-\\mu)^j)$ ne their common central $j^{th}$ moment. Then the mean $\\mu_{|x|}$ and the variance $\\sigma_{|x|}^2$ of the Euclidena norm are:\n",
    "\n",
    "$\\mu_{|x|} = \\sqrt{aD -b} + O(D^{-1})$\n",
    "$\\sigma_{|x|}^2 = b + O(D^{-1/2})$\n",
    "\n",
    "Variance grows linearly -> random vectors are nearly normalized. \n",
    "\n",
    "If you pick up a set of random vectors in d dimension all random vectors are close to the surface of a sphere of radius r. -> Euclidean distance between any two vectors is approximately constant. \n",
    "\n",
    "INTRINSIC/EXTRINSIC DIM \n",
    "\n",
    "The points of high dimensional data usually reside on a much low-dimensional manifold\n",
    "$ X = {x_{\\alpha}}_{\\alpha \\in A}, x_{\\alpha} \\in M, dim(M) = S$\n",
    "D extrinsic dimension of X, s intrinsic dimension of X\n",
    "X sample set of random vectors X in dimension D\n",
    "there exist a random vector Y in dim s and an invertible anlytic function such that $f(Y) = X$\n",
    "\n",
    "Due to the low intrinsic dimension of data, we can reduce the extrinsic dimension wihtout losing much information for many types of real-life high dimensional data, avoiding many of the curses of dimensionality. \n",
    "DR is finding a parametrization of the manifold which the points of data reside on. \n",
    "\n",
    "INTRINSIC DIM EXTIMATE\n",
    "\n",
    "A finite data set can be embedded in a manyfolds of various dimension depending on the geometric structures defined on the data. E.g. by unisolvence theorem, the dataset here on the plane can be embedded in a regular curve. \n",
    "\n",
    "LINEAR CASE \n",
    "simplest manifold: linear subspace -> hyperplane. Among all hyperplanes S including the dataset X, choose the one with lowest dimension s. There exists a linear transformation $T : R^s -> S$. \n",
    "-> $x = T(y) = U_y + x_0$\n",
    "where U is orhognal and $X_0$ is then earest point on S to the origin. \n",
    "The inverse of T provides a s-dim paramtetrisation of X.\n",
    "$Y = {y \\in R^S: y = T^{-1}(x), x \\in X}$\n",
    "wlog $x_0 = 0$ and if $U = [u_1, ..., u_s]$ then  $ {u_1,..,u_s}$ is an orthonormal basis of S. \n",
    "Distances are preserved and we have an exmaple of linear dr. \n",
    "\n",
    "NON LINEAR CASE\n",
    "\n",
    "PROBLEM: if the underlying geometry is non linear, the previous approach does not give a true dimension. We then use the topological manyfold theory. \n",
    "If a manifold M in $R^D$ has dimesnion $s<D$, then the neighbourhood of each point of M is isomorrphic to $R^s$, there is an invertible differentiable map from M to $R^s$ whose inverse if differentiable.\n",
    "\n",
    "Given a space S, the open covering of X in S is a collection of O of open sets in S whose union contains X. \n",
    "A refiniment of O is another covering O' such that each set in O' is a subset of some set in O.\n",
    "An S-dimensional set X can be covered by open spheres such that each point belongs to at most s+1 open spheres. \n",
    "A subset X of a topological space S is said to have topological dimension s if every covering O of X has a refinement O' such that every point of X is covered by at most s+1 open sets in O' and s is the smallest among such integers.\n",
    "\n",
    "-> DR methods\n",
    "In most dimensionality reduction problems, the output data is not required to have the intrinsic output dimension s but a target dimension d that is lower than the intrinsic one, d<s. \n",
    "\n",
    "* Linear: output data are a projection of the original data (PCA MDS)\n",
    "* Non-Linear: output dtaa are the manifold coordinate representation of the original data (UMAP t-SNE isomap)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d97ff0",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Linear Case  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736d4d9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21bf9610",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc038b1",
   "metadata": {},
   "source": [
    "The PCA is probably the most famous algorithm used for dimensionality reduction. The main idea of PCA as for the other method of dimensionality reduction is to reduce the number the dimensionality retaining as mach variability as possible.\n",
    "The idea behind the Principal Component Analysis (PCA) is to project orthogonally a dataset $X = [x1,...xn]$ of np dimensional points into a r-dimensional space with $r = min(n-1,p)$, so that in the new coordinates the projected point' variables are uncorrelated. \n",
    "\n",
    "The new coordinates are called _principal components_, and each component is defined by the rules: being orthogonal to the previous component having highest possible variance. These novel coordinates form an orthogonal basis. \n",
    "So, PCA consists in fitting a p-dimensional ellipsoid to the data; each axis of the ellipsoid is a principal component.\n",
    "\n",
    "PCA algorithm was introduced by pearson\n",
    "\n",
    "The schema of the PCA is the follwoing:\n",
    "* center data;\n",
    "* comput covariance matrix;\n",
    "* extract eigen-values(vectors;\n",
    "* normalize eigenvectors;\n",
    "* diagonalize covariance matrix with variance on diagonal;\n",
    "* compute the proportions of variances\n",
    "\n",
    "_SETUP_: Consider $D = {z_{i},..., z_{n}}$ dataset of n samples in p variables: $z_{i} = (z_{i1}, ..., z_{ip})$ center varibales: $x_{ij} = z_{ij}- (1/n) \\sum{i z_{ij}}$.\n",
    "Obtain $X$ in $R^{nxp}$ - data matrix columnwise zero centered. \n",
    "Goal: transform $X = {x_{1}, ..., x_{n}}$ into a new dataset $T = {t_{1}, ..., t_{n}}$ in l variables such that sample (row) $x_i$ is mapped into the sample (row) $t_i$ by the matrix: $t_{ik} = x_{i}*w_{k}$.\n",
    "\n",
    "Such that $t_i,...,t_l$ iherit the maximum possible variance from X and $w_{k} = (w_{1k},...,w_{pk})  has norm one for each $k =1,...l$\n",
    "-> PCA depends on the choosen scaling. \n",
    "\n",
    "In this way we are able to compute the components: the 1st component is the one which explains the biggest amount of variance inside the dataset and it corresponde to the eigenvector associated to the largest eigenvalue. Geometrically, the first principal components corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. \n",
    "\n",
    "The algorithm of the pca can be done in three different ways according to the kind of data that we have:\n",
    "* covariance, when we have data with similar scales;\n",
    "* correlation, when we have data with different scales;\n",
    "* singular values decomposition, general purpose procedure. \n",
    "\n",
    "_EXPLAINED VARIANCE_: the first principal component correponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line. Each eigenvalue is proportional to the proportion of the 'variance' (more correctly of the sum of the squared distances of the points fromm their multidimensional mean) that is associated with each eigenvector. \n",
    "The sum of all the eingenvalues is equal to the sum of the squared distances of thep oints from their multidimensional mean. \n",
    "Explained variance of the k-th pc: $\\lambda_{k}/\\sum_{\\substack{i}}{\\lambda_{k}}$\n",
    "PCA essentially rotates the set of points around their mean in order to align with the principal components. This moves as much of the variance as possible into the first few dimensions.\n",
    "\n",
    "_A DIFFERENT OBJ FUNCTION_: PCA minimizes low-dimensional reconstruction error. Alternative: maximize the scatter of the projection, so to obtain the most informative projection; this is known as classical scalinga. \n",
    "\n",
    "A drawback of PCA is that it cannot caputre the small variation in the data, it is onlt able to detect bug changes in the variance. Moreover, pca can shows a relation that does not exist as in the case of the double swiss roll. \n",
    "\n",
    "MDS refers to a set of related ordination techniques used in information visualization, in particular to display the information contained in a distance matrix. It is a form of non-linear dimensionality reduction.\n",
    "Thus, we can say the PCA should perform better in cases of linear dimensionality reduction, so when data are low dimensional and they require at max two-three dimension. \n",
    "\n",
    "__EXAMPLE ???__\n",
    "An example where we can prefer PCA over MDS is the one of the USArrests data set. For each of the 50 states in the United States, the data set contains the number of arrestsper 100000 residents for each of three crimes: Assault, Murder, and Rape. We also record UrbanPop (the percent of the population in each state living in urban areas). The results of the PCA on this dataset show that the first loading vector places approximately equal weight on Assault, Murder, and Rape, with much less weight on UrbanPop. Hence this component roughly corresponds to a measure of overall rates of serious crimes. The second loading vector places most of its weight on UrbanPop and much less weight on the other three features. Hence, this component roughly corresponds to the level of urbanization of the state.\n",
    "Overall, we see that the crime-related variables (Murder, Assault, and Rape) are located close to each other, and that the UrbanPop variable is far from the other three. This indicates that the crime-related variables are correlated with each other—states with high murder rates tend to have high assault and rape rates—and that the UrbanPop variable is less correlated with the other three."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdcb3ad",
   "metadata": {},
   "source": [
    "### PCA & t-SNE pros & cons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1d538f",
   "metadata": {},
   "source": [
    "__CONS__\n",
    "* PCA: requires more than 2 dimensions; Thrown off by quantised data; Expects linear relationships.\n",
    "* t-SNE: Can't cope with noisy data; Loses the ability to cluser\n",
    "__PROS__\n",
    "* PCA: good at extracting signal from noise; Extracts informative dimensions.\n",
    "* t-SNE: can reduce to 2D well; Can cope with non-linear scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7883d23f",
   "metadata": {},
   "source": [
    "## MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "774b6ace",
   "metadata": {},
   "source": [
    "Multidimensional scaling is a family of algorithms visualizing the level of similarity of individual cases of a dataset. It can be seen as a derivation of PCA where you try to keep the distances between points in the projected space as close as possible to the orginal distances in the original space.\n",
    "In practice, mds finds an embedding of n objects into a r-dimensional euclidean space $\\ R_n$ so to preserve as well as possible (a function of) the distances between original objects.\n",
    "mds can be distinguished in 3 kinds, depending on the objective function:\n",
    "\n",
    "• classical mds: the objective function is called strain and involves directly the original distances between objects.\n",
    "\n",
    "• metric mds: the objective function is called stress and involves a function of the original distances.\n",
    "\n",
    "• non-metric mds: the original distances are dissimilarities, so the stress function finds a non-parametric monotonic relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items. Than defines the location of each item.\n",
    "\n",
    "In the classical mds case, the solution is deterministic so that given data-points and space structure you can find (the analytic solution) your output-points. The dimensionality reduction core is the same as PCA and in metric and non-metric mds the output is obtained through maximization.\n",
    "\n",
    "_DISTANCES_\n",
    "Let $\\ O=\\{o_1,…,o_n\\}$ be a dataset of n objects with a dissimilarity measure $\\ d_{ij}$\n",
    "mds aims at finding n points $\\ x_1,…,x_n$ in $\\ R^p$ so that $\\ ||x_i-x_j||≅d_{ij}$ .\n",
    "If for some $\\ p ||x_i-x_j|=d_{ij}$, $d$ is called an euclidean distance and if no such $p$ exists but $d$ is a metric, $d$ is called a non-euclidean distance.\n",
    "Simple example of non euclidean space: lenght of the smallest path from two points in a circle. \n",
    "\n",
    "For eculidean distances the algorithm's solution is unique but for non euclidean distances only approximation is possible.\n",
    "For dimensionality reduction, since the first $p<q$ components best preserve the distances among all other p-dim reductions, one can use these p components to obtain a lower dimensional projection of the data. Later we can pick a smaller amount of components.\n",
    "\n",
    "\n",
    "Alternative mds algorithms relax the objective $\\ ||x_i-x_j||≅d_{ij}$ to $\\ ||x_i-x_j||≅f(d_{ij})$ (a function of the distance in the original space). Whether we implement metric or non-metric mds still depends on $d$ being quantitative or not (could be ordinal). The problem now  becomes an optimisation process aimed at minimising a stress function which is solved by iterative algorithms.\n",
    "\n",
    "_METRIC MDS - SAMMON MAPPING_\n",
    "A metric method is Sammon mapping and an example of stress function for summon mapping is:\n",
    "$\\ ℒ = \\frac{1}{\\sum \\limits _{l<k1} d_{lk}}\\sum \\limits_{i<j} \\frac{(| | x_i − x_j | | − d_{ij})^2}{dij}$\n",
    "Sammon mapping better preserves the small $d_{ij}$, giving them a freater degree of importance in the fitting proceduere than for larger values of $d_{ij}$\n",
    "Sammon mapping better preserves  inter-distances for smaller dissimilarities, while proportionally squeezes the inter-distances for larger dissimilarities. \n",
    "\n",
    "_NON-METRIC MDS_\n",
    "Non-metric mds is instead implemented when dissimilarities are known only by their rank order, and the spacing between successively ranked dissimilarities is of no interest or is unavailable.\n",
    "here $f$ is only implicitly defined as a regression curve, and only preserves the order of $d$, that is:\n",
    "$\\ f(d_{ij})<f(d_{kl}) if d_{ij}<d_{kl}$ \n",
    "Thus only the order of $d$ is needed, not the actual values. \n",
    "The most common algorithm is the Kruskal mds.\n",
    "\n",
    "\n",
    "_MDS OUTPERFORMS PCA ?_\n",
    "In conclusion mds is looking for similarities between data points by computing the Gram matrix $\\ G ∈ R_{n×n}$ .\n",
    "It turns out to be more efficient than PCA if you have few data points in very high dimension (for example\n",
    "you can have 100 images represented in $\\ R^{d≃10^{6}}$, each coordinate corresponding to a given pixel).\n",
    "mds can also be extended to be applied in metric spaces (with no explicit representation of your data in $\\ R_d$).\n",
    "\n",
    "An example of metric mds can be when we want to reduce dimensioanlity of a dataset where the response variable is whether a person owns or not a house and we take in considerations many variables such as his wage, age and so on. We can compute euclidean distances between these points because the variables are continuous and their distance in space is extremely relevant for the analysis.\n",
    "For non-metric mds instead we have an ordinal distance measure therefore this could be ranking individuals by their degreee of kinship where the spacing between successively ranked dissimilarities is of no interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee205195",
   "metadata": {},
   "source": [
    "__ECKMAN COLORS__: 14 colors, different only in hue (wavelenght), pairwise rated as similar in a 0-5 scale by 31 people. We obtain a dissimilarity matrix 14x14 where the similarity votes have been scaled. If we use a non-metric mds, we project on the plane the points so that the distances are preserved in this plane, we obtain a results that resembles the color circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249ba124",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction - Non-Linear case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2601b316",
   "metadata": {},
   "source": [
    "### TOPOLOGICAL FACTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e39be487",
   "metadata": {},
   "source": [
    "TOPOLOGICAL SPACE an ordered pair $(X. \\tau)$ where $X$ is a set and $\\tau$ is a collection of subsets of $X$, called topology, for which the following axioms hold: ${\\phi, X} \\subset X$. Any arbitrary union of elements in $\\tau$  belongs to $\\tau$. The elements of $\\tau$ are called open sets. Example: $R^n$ with $\\tau$ any union of open balls. \n",
    "\n",
    "MANIFOLD: it is a topological space resembling euclidean space near each point. \n",
    "\n",
    "COVER: a collection of subsets of $X$ whose union is X\n",
    "\n",
    "HOMOTOPY : informally, it is a continuous deformation between f and g.\n",
    "\n",
    "HOMOTOPY EQUIVALENCE: two topological spaces $X,Y$ are homotopy if $\\exists f:X\\to Y$ and $g: Y \\to X$ continuous such that $f g$  is homotopic to $1x$ and $gf$ is homotopic to $1y$. Every homeomorphism is an homopy equivalence, but not the converse. Namely, two spaces $X$ and $Y$ are homotopy equivalent if they can be transoformed into one another by bending, shrinking and expanding operations.\n",
    "\n",
    "CONTRACTIBLE SPACE: hotompy equivalent to a point\n",
    "\n",
    "SIMPLICES: (k-)simplex, a k-dim polytope that is the convex hull of its k+1 vertices; a regular n-simplex can be constructed from a regular (n-1) simplex by connecting a new vertex to all original verices by the common edge length. \n",
    "SIMPLICIAL COMPLEX: set K of simplices glued together along faces: any face of any simplec in K is also in K and the intersection of two simpleces in K is also K.\n",
    "\n",
    "CECH NERVE: given an open cover $\\upsilon = (U_i)_{i \\in l}$ of a topological space X, consider the fibre product $U_{ij} = U_i X_x U_j = U_i \\bigcap U_j$; Generalizing to arbitrary intersection we obtain the simplicial object $N(\\upsilon) = X_X U$ Called the cech nerve (complex) of X. \n",
    "\n",
    "In practice, let each set in the cover be a 0-simplex, create a 1-simplex between two such sets if they have a non empy intersection, create a 2-simplex between such sets if the triple intersection of all three is non-empty and so on. \n",
    "\n",
    "_compact space_ : a topological space X is compact if each of its open cover has a finite subcover.\n",
    "_nerve theorem_ if X i compact and $\\upsilon = (U_i)_{i \\in l}$ is an open cover such that for each $\\sigma \\subseteq l$, the intersection is either contractible or empy, then $N(\\upsilon)$ is homotopy equivalent to X.\n",
    "Thus, from the nerve of X we can actually recover all the key topological structures of the original space.\n",
    "\n",
    "FROM SPACES TO DATA\n",
    "to apply the above construction to a dataset, a open cover is needed. If data lie in a metric space (i.e there exist a distance) the open cover can be obtained by the balls centered in each data point, thus there is a 0-simplex for each data point. \n",
    "Note that most of the job can be done just considering 0- and 1-simpleces: mathematically, this is equivalent to consider the victories rips complex instead of the cech complex. This yields that a dimensional reduction can be obtained by mean of a graph representation. \n",
    "\n",
    "-> ISSUES: \n",
    "* Problem 1: what's the optimal radius for the open cover? too small -> too many connected components; to large: few very high dimensional simplices, and no structure\n",
    "* Probelm 2: if points are too scattered, the structure cannot be properly captured. Ifp oints are too dense, the cover generates a too high dim nerve.\n",
    "\n",
    "Best possible situation: having data points uniformly distributed across the manifold radius -> half the average distance between points no gaps and no clumps in the cover. \n",
    "Solution -> adapt the notion of distance on the manifold so that all the points seem to be uniformly distributed. -> Solution STRETCH THE SPACE.\n",
    "Thus, give to each point its own unique distance function and select balls of radius one with respect to that local distance function.\n",
    "The unit ball about a point stretches to the k-th nearest neighbor of the point, where k is the sample size we are using to approximate the local sense of distance. \n",
    "Fuzziness -> given the local metric, we can weight the edges of the graph according to the edge'verices distance. In this way, points may end up being separated by the rest of the manifold. Moreover, local connectivity is introduced: the fuzziness decays only beyond the nearest neighbour. The focus is on the difference in distances among nearest neighbors rather than the absolute distance, avoid the curse of dimensionality. \n",
    "Incompatibility -> since each point has it own metric, distance from a to b may be different than distance from b to a ! The thoritacally grouded solution is correctly defining the fuzzy union of simplicial sets as the probability thta at least one of the edge exists, thus ending with a single fuzzy simplicial complex (weighted graph). \n",
    "\n",
    "Now we have strucutred our original dataset in a graph that represent the geometrical structure. Now we need to project it into a low dimensional euclidean space to preserve the orignal structure of the manifold. Mathematically, this means deciding which $f(w_h, w_l)$ to optimize, where $w_h, w_l$ are the graph edges' weights in high and low dimension. \n",
    "Sunce $w_h, w_l$ are bernoulli variables, the correct function to optimize is the _cross entropy_. The cross entropy is mainly compose by two parts: \n",
    "* one attractive force between points when w is large in high dimension-> optimizing clumps\n",
    "* one repulsive force between points when w is small in high dimension -> optimizing gaps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef255ed9",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0fa1f07",
   "metadata": {},
   "source": [
    "## t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d71480f",
   "metadata": {},
   "source": [
    "### Intro to t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e8e7a7",
   "metadata": {},
   "source": [
    "__pca problems__\n",
    "PCA problems: PCA targets dimensionality preserving large pairwise distances in the map, but cannot catch the structure of the data. \n",
    "The swiss roll data show quite clearly the problem of dimensionality reduction with PCA. PCA explains 90% of variance, but PCA determines relation that does not exist. \n",
    "-> Solution t-SNE$\\sim$t-distirbuteed stochastic neighbor embedding. The main idea behind this approach is that what is reliable in DR are the very small euclidean distances between neighbouring points. What determines the overall structure is the concept of neighbourhood. \n",
    "\n",
    "__t-sne in brief__\n",
    "Let's suppose to have a multidimensional dataset, we want to use a gaussian function to weight the distances between points. Construct a smoothing distance function to construct the concept of metric neighbourhood. \n",
    "t-SNE method-> construct a map project this high dimensional space into a low dimensional space. Here the idea is to define properly the distances in the two spaces. In the original space we use a gaussian distance, in the low space we use instead a t-student distribution.  The core of t-SNE algorithm is to find theminimum of the Kullback-leiber divergence $C = KL(P||Q) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} p_{ij} \\log{\\frac{p_{ij}}{q_{ij}}} $\n",
    "\n",
    "__novelty__\n",
    "Novelty of t-SNE with respect to previous sne algorithms:\n",
    "* tsne uses joint instead of conditional probabilities, this introduces symmetry in the problem formulation, and the cost function optimisation is computationally much simpler. \n",
    "* uses t-student distirbution instead of gaussian modelling on the owe dimensional space: this heavy-tailed distribution in the low dimensional space alleviates both the crowding problem and the optmization problem \n",
    "_crodwing problem_ the volume of a sphere centered on datapoint i sclase as $r^m$, where is the radius and m the dimensionality of the sphere; if we want to madel the small distances accurately in the map, most of the points that are at a moderate distance from datapoint I will have to be placed much too far away in the two dimensional map. \n",
    "\n",
    "__t-student on low dim space__\n",
    "The heavy tailed distirbution instead corrects volume differences between both spaces.\n",
    "In fact, it allows to modereate distance in the high dimensional space to be faithfully modeled by a much larger distance in the map and as a result it eliminates the unwanted attractive forces between map points that represent moderately dissimilar data points. \n",
    "This is due to the fact that the map's representation of joint probabilities is (almost) invariant to changes in the scale of the map for map points that are far apart. \n",
    "__Core algorithm - Optimization step__\n",
    "Since the kullback-leiber divergence is not symmetric, different types of error in the pairwise distances in the low-dimensional map are not weighted equally. \n",
    "There is a large cost for using widely separated map points to represent nearby data points, but there is only a small cost using nearby map point to represent widely separated data points. \n",
    "Solution is found by using gradient descent. In details, one takes consecutive steps proportional to the negative of the gradient of the function at the current point.\n",
    "__tsne optimization__\n",
    "n bodeis elastic system -> Strongly repels dissimilar data points that are modeled by a small pairwise distance in the low-dimensional representation. \n",
    "These repulsion do not go to infinity. \n",
    "* Early compression: force the map points to stay close together at the start of the optmization, implemented by adding an additional l2-penalty to the cost function thay is proportional to the sum of sqaures distances of the map points from origin.\n",
    "* Early exageration: multiply all of the $p_{ij}$'s by, for example, 4, in the initial stages of the optimization, modelling the large $p_{ij}$'s by fairly large $q_{ij}$'s natural clusters in the data tend to form tight widely separated clusters in the map. \n",
    "__Perplexity__\n",
    "The only parameter to set is the variance $\\sigma_i$ for the high dim gaussian of $p_{ij}$. \n",
    "No single value of $\\sigma_i$ can be optimal for all data points in the dat set becasue the density of the data is likely to vary. \n",
    "In dense regions, a smaller value of $\\sigma_i$ is usually more appropriate than in sparse regions. \n",
    "Any particular value of $\\sigma_i$ induces a probability distribution $P_i$ that has an entropy which increases as $\\sigma_i$ increases- \n",
    "t-SNE performs a binary search for the value of $\\sigma_i$ that produces a $P_i$ with a fixed perplexity that is specified by the user:\n",
    "$Perp(P_i)= 2^{H(P_i)} = 2^{-\\sum_{i}P_{ij}log_{2}{P_ij}}$\n",
    "Perplexity can be interpreted as a smooth measure of the effective number of neighbors, typical values are between 5 and 50. \n",
    "\n",
    "__Examples__\n",
    "fashion mnist -> t-sne performs better than sammon mapping and pca. \n",
    "coil20 dataset -> the same \n",
    "double swiss roll -> 3d manifold -> pca & classical mds perform poorly, sammon non linear mapping starts differentiating the two layers while t-SNE does a quite good job although it mixing up the two layers.\n",
    "\n",
    "__Drawbacks__\n",
    "* Does not output transformation\n",
    "* stochastic\n",
    "* output space hard to interpret\n",
    "* hyperparameter choice critical\n",
    "* views at different perplexities needed to understand topology \n",
    "* artifacts in data may appear\n",
    "* cluster size and inter- cluster distance not meaningful\n",
    "* slow on large dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873f74a7",
   "metadata": {},
   "source": [
    "Before introducing the tSNE, it is necessary to expalin the problems connected to high dimensional data. Indeed, we need to reduce because:\n",
    "* many data analysis pipeline cannot be used for high dimensional data;\n",
    "* instability and/or computational issues make them unreliable \n",
    "* need for dimension reduction for example when we want to make image classification. \n",
    "* lastly, it is impossible to visualize data when there are more than 3 dimension. \n",
    "To solve all the above issues have been implemented different algorithms in order to reduce the dimensionality of the data retaining as much information as possible. \n",
    "t-SNE is a method adopted to perform dimensionality reduction. \n",
    "The main basic idea of t-SNE is that _what is reliable in dimensionality reduction are the very small  distances between neighbouring points_. \n",
    "In simple words what t-SNE does is find a way to project data into a low dimensional space so that the clustering in high dimensional space is preserved. \n",
    "So what t-SNE does is measuirng distances between points, the idea is to define properly the distance among the points. The core of t-SNE algorithm is to find the minimum of the _kullback-leiber function_ : $C = KL(P||Q) = \\sum_{i=1}^{n} \\sum_{j=1}^{n} p_{ij} \\log{\\frac{p_{ij}}{q_{ij}}} $\n",
    "\n",
    "The novelty that the t-SNE method brought are the following:\n",
    "* t-SNE uses joint instead of conditional probabilities -> this introduce symmetry -> function optimisation is easier\n",
    "* uses the t-student distribution instead of the gaussian on the low-dimensional space: this alleviates crowding probelm and the optimization problem. The t-student on low dimensional space eliminates the unwanted attractive forces between map points that represent moderately disismilar data points. This is due to the fact that the maps's representation of joint probabilities is (almost) invariant to changes in the scale of the map for map points that are far apart. \n",
    "\n",
    "The critical parameter to set when performing the t-SNE is the _perplexity_, the variance of $\\sigma_{i}$  for the high-dim gaussian of $p_{ij}$. Indeed, no single value of $\\sigma_{i}$ can be optimal for all data points in the data set because of the data is likely to vary. \n",
    "\n",
    "__WHEN BETTER THAN MDS__\n",
    "\n",
    "__Multidimensional scaling__ is a family of algorithms visualizing the level of similarity of individual cases of a dataset. As mapping, PCA is a particular case of MDS. \n",
    "In practice, mds finds an embedding of n objects into a r-dimensional euclidean space ${R^n}$ so to preserve as well as possible (a function of) the distances between original objects. \n",
    "MDS can be distinguished in 3 kinds, depending on the objective function: \n",
    "* classical mds- the objective function is called strain and involves directly the original distances between objects\n",
    "* metric mds- the object function is called stress and involves a function of the original distances \n",
    "* non-metric mds- the original distances arre dissimilarities, so the stress function finds a non-parametric monotonic relationship between the dissimilarities in the item-item matrix and the Euclidean distances between items, and defines the location of each item. \n",
    "\n",
    "Considering the PCA as a method belonging to the mds, we can say that t-sne will perform better in cases where we have to perform the dimensionality reduction on the swiss roll data. In this case, PCA is not able to capture small variation in the data. It is only able to detect big changes in the variance. Indeed, in the case of the reduced swiss roll with PCA there may seem to be relations that do not exist. In this case, t-SNE will perform better. \n",
    "\n",
    "Nevertheless, t-SNE is not perfect. Indeed, it is:\n",
    "* hard to interpret in terms of geometry. \n",
    "* choice of hyperparameters is critical \n",
    "* although does a good job in clustering, the size of the cluster and the distance between cluster is not meaningful\n",
    "* slow on large datasets \n",
    "* does not output transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5767ee3",
   "metadata": {},
   "source": [
    "### Perplexity\n",
    "Perplexity = expected number of neighbours within a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe014e4",
   "metadata": {},
   "source": [
    "## UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82e7d10",
   "metadata": {},
   "source": [
    "Try to detect the manyfold- the geometrical structure and trying to approximate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4ea593",
   "metadata": {},
   "source": [
    "UMAP aka Uniform manyfold Approximation and projection. \n",
    "The last dimensionality reduction method is __UMAP__, uniform manifold approximation and projection. \n",
    "The underlying idea behind umap is to detect the manyfold geometrical structure where the points are leaving and tr to approximate it and project it in a two (three) dimensional space. \n",
    "Umap is basically done in two main steps: the graph construction (high dimension) and the graph projection (low dimension). \n",
    "The cool part of umap is that its steps are mathematically proven to work. \n",
    "Firstly, we have the data in high dimensional space and we want to approximate their shape/topology. Each point is called 0-simplex. \n",
    "Nerve theorem -> shape of data can be approximated when we connect the 0 simpleces with their nighbouring data point forming 1,2 or higher simpleces. With this we can approximate the topology. \n",
    "So what we need to do is making this connections. For this, the umap algorithm extends a radius around each point and makes a connection between each point and its neghbors with interseting radii. \n",
    "So far the radii are equal. But rember, we want to approximate the shape of the data, so we want a connceted graph containing all our data points. But this whish brings in two problems:\n",
    "* Firstly, it oftne happens that in the data there are larger gaps, where there is no next point to connect to in the graph. This usually happens in low density regions. \n",
    "* Secondly, there are often high density regions where there are a lot of neighbors in the given radius and everything is way to connected. \n",
    "The second problem becomes even worse with the _curse of dimensionality_, where in high dimensional spaces the distances between points become more and more similar. \n",
    "Thus, we can use instead equal radius, variable radius. \n",
    "This choice is mathematically supported by the definition of a _Riemannian metric_ on the manifold. \n",
    "To recap, in small density regions the radius is bigger while in high in high density regions the radius is smaller. \n",
    "Moreover, umap does not estimate density directly as a number but uses a proxy: the density is estimated to be higher when the kth neighbor is close and lower when the kth nearest neighbor is far away.\n",
    "As we know, the k is an hyperparameter that needs to be chosen:\n",
    "* k big -> more global structure is preserved\n",
    "* k small -> the radius decreases and the local structure is more preserved. \n",
    "So we need to wisely choose the k in order to achieve the perfect balance between the local and the global structure preservation.\n",
    "Unfortunately, there is not a magic receipes that allows us to find th optimal k automatically. We need the trial and error procedure. \n",
    "But not all k neighbors are equal, since each have different distances from the point we are looking at. Then the connections between each point and their neighbors get a weight, a connection probability, where points which are far away, are weighted less and get lower connection probability. \n",
    "\n",
    "Now our dimensional graph is constructed and it is ready to be projected to lower dimensions. We can imagine this projection as taking the high dimensional graph, with their edges being springs, where each spring is stronger as the edge probability increase. \n",
    "This means that poinys connected by high weighted edges are more likely to stay together in the lower dimensional space, because the spring holds these points together. And perhaps it is interesting to notice that soring forces are rotationally symmetric which lead to clusters sometimes landing on one side after one umap run and on the other side adter another projection. \n",
    "\n",
    "Compared to t-SNE, UMAP:\n",
    "* is faster due to its optimizations and strong mathematical foundations,\n",
    "* has a better balance between locality and globality in clustering. \n",
    "\n",
    "However, UMAP excels at reducing from a lot of dimensions. The most common example where we can compare the higher performance of UMAP compared to PCA occurs when we want to perform dimensionality reduction on the mnist dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c93427f",
   "metadata": {},
   "source": [
    "### UMAP Vs t-SNE\n",
    "UMAP is a replacement for tSNE to fulfil the same role. \n",
    "Conceptually very similar to tSNE, but with a couple of relevant chenges.\n",
    "UMAP is quicker than t-SNE, can preserve more global strcuture than t-SNE (at least theorically, can run on raw data without PCA preprocessing (in theory), can allow new data to be added to an existing projection. \n",
    "\n",
    "__DIFFERENCES__\n",
    "Instead of the single perplexity value in tSNE, UMAP defines:\n",
    "* Nearest neighbours: the number of expected nearest neighbours- basically the same concept as perplexity;\n",
    "* Minimum distance: how tightly UMAP packs points which are close together. \n",
    "Nearest neighbours will affect the influence given to global vs local information. Min dist will affect how compactly packed the local parts of the plot are. \n",
    "\n",
    "UMAP may perform better on more complex datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ba3153",
   "metadata": {},
   "source": [
    "### UMAP Hyperparameters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9760e5af",
   "metadata": {},
   "source": [
    "* n_neighbors: determines the number of neighboring points used in local approximations of manifold structure. Larger values will result in more global structure being preserved at the losso of detailed local structure. Should often be in the range 5 to 50, with a choice of 10 to 15 being a sensible default. \n",
    "* min_dist: controls how tightly the embedding is allowed compress points together. Larger values ensure embedded points are mpre evenly distribute, while smaller values allow the algorithm to optimise more accurately with regard to local structure. Sensible values [0.0001, 0.5], with 0.1 being a reasonable default.\n",
    "* metric: determines the choice of metric used to measure distance in the input space. Wide variety of metrics are already coded. A user defined function can be passed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed7e3e",
   "metadata": {},
   "source": [
    "__UMAP VS t-SNE__\n",
    "\n",
    "Nice thing that UMAP allows is constructing a real projection of the space. \n",
    "t-SNE only maps my points input in  space to my projected points in the target space, the function of UMAP is a real transformation map. You can transform all the space where the points leave. So I can use a training set and a test set where points are only projected. \n",
    "\n",
    "Among the pros of UMAP we have very good performances also in big dataset. \n",
    "\n",
    "As implementation, we need to use an optimization algorithm, the ones that have been used are the nearest neighbour descent, the stochastic gradient descent, the probabilistic edge sampling, negative sampling. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0bbb91",
   "metadata": {},
   "source": [
    "## Linear Vs Non-Linear Dimensionality Reduction  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4e1c1b",
   "metadata": {},
   "source": [
    "The goal of dimensionality reduction is to reduce the dimensionality of the data to allow efficient visualisation of the dataset, greater understanding of the distribution of data, and summrise the information.\n",
    "\n",
    "Infact, the points of high-dimensional data usually reside on a much lower-dimensional manifold. The dimesnion of this manifold is called intrinsic dimension of the data. Due to the low intrinsic dimension of data, we can reduce the (extrinsic) dimension without losing much information for many types of real-life high-dimensional data, avoiding many of the curses of dimensionality.\n",
    "\n",
    "There are many algorithms that can implemented and that can accomplish sich task. One possible division is between linear and non linear algorithms.\n",
    "\n",
    "In  the linear case a linear subspace of the original data is used as manifold. The idea is to choose, among all hyperplanes S including the dataset X, the one with lowest dimension s.\n",
    "there exists a linear transformation $\\ T : ℝ^s → S$\n",
    "\n",
    "$\\ x = T(y) = U_y + x_0$\n",
    "\n",
    "where U is orthogonal $(<u,v>=<U_u,U_v>)$ and $\\ x_0$ is the nearest point on S to the origin. The inverse of T provides a s-dim parameterisation of X:\n",
    "\n",
    "$\\ Y = {y ∈ ℝ^s : y = T^{-1}(x), x ∈ X}$\n",
    "\n",
    "without loss of generality x0=0 and if U=$\\ [u_1,...,u_s]$, then $\\ \\{u_1,…,u_s \\}$ is an orthonormal basis of S, each column is orthogonal with norm equal to 1, so that distances are preserved and we have an example of linear dimensionality reduction.\n",
    "\n",
    "if the underlying geometry is non linear, the previous approach does not give a true dimension. Therefore we then use the topological manifold theory.\n",
    "If a manifold M in $\\ R^D$ has dimension s<D, then the neighbourhood of each point of M is isomorphic to $\\ R^s$ and there is an invertible differentiable map from M to $\\ R^s$ whose inverse if differentiable.\n",
    "\n",
    "So, given a space $S$, the open covering of $X$ in $S$ is a collection $O$ of open sets in $S$ whose union contains $X$ and a refinement of $O$ is another covering $O'$ such that each set in $O'$ is a subset of some set in $O$ .\n",
    "Follows that an s-dimensional set $X$ can be covered by open spheres such that each point belongs to at most $s + 1$ open spheres.\n",
    "A subset $X$ of a topological space $S$ is said to have topological dimension s (also called Lebesgue covering dimension) if every\n",
    "covering $O$ of $X$ has a refinement $O′$ such that every point of $X$ iscovered by at most $s + 1$ open sets in $O′$, and $s$ is the smallest among such integers.\n",
    "\n",
    "In conclusion, in the linear case output data are a projection of the original data output and principal component analysis and multidimensional scaling are linear dimensionality reduction algorithms, while on the non-linear case output data are the manifold coordinate representation of the original data and tsne and isomap are non-linear dimensionality reduction algorithms.\n",
    "\n",
    "In the case of the double swiss roll tsne has much better performs than pca because in seperating classes because the structure of the data is higly non linear and tsne is capable of detecting the structure of the data in the contrary of PCA beacuse it focuses on neighbourus rather than on high variations. Infact pca targets dimensionality, preserving large pairwise distances in the map, but cannot catch the structure of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483a9cfe",
   "metadata": {},
   "source": [
    "## PCA vs UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f49b77e1",
   "metadata": {},
   "source": [
    "Both PCA and UMAP are dimensionality reduction algorithms.\n",
    "The goal of dimensionality reduction is to reduce the dimensionality of the data to allow efficient visualisation of the dataset, greater understanding of the distribution of data, and summrise the information. Infact, the points of high-dimensional data usually reside on a much lower-dimensional manifold. The dimesnion of this manifold is called intrinsic dimension of the data. Due to the low intrinsic dimension of data, we can reduce the (extrinsic) dimension without losing much information for many types of real-life high-dimensional data, avoiding many of the curses of dimensionality.\n",
    "There are many algorithms that can implemented and that can accomplish such task. One possible division is between linear and non linear algorithms. PCA is a linear algorithm, while UMAP is not.\n",
    "\n",
    "**PCA**\n",
    "\n",
    "The idea behind PCA is to project orthogonally a dataset $\\ X=\\{ x_1,…,x_n \\}$ of n p-dimensional points into a r-dimensional space\n",
    "with $\\ r = min(n-1,p)$, so that in the new coordinates the projected points variables are uncorrelated.\n",
    "the new coordinates are called principal components, and each component is defined by the rules:\n",
    "• being orthogonal to the previous components\n",
    "• having highest possible variance\n",
    "The novel coordinates form an orthogonal basis.\n",
    "Therefroe the first principal component catches most of the variability of the dataset, the one second catches the second axis where most variaiblity lays and so on forth.\n",
    "The step to follow are: center data, compute variance-covariance matrix, extract eigenvalues and eigenvectors, normalize vectors, diagonalize covariance matrix with variance on diagonal and compute proportions of variances.\n",
    "The first principal component corresponds to a line that passes through the multidimensional mean and minimizes the sum of squares of the distances of the points from the line so PCA essentially rotates the set of points around their mean in order to align them with the principal components. This moves as much of the variance as possible (using an orthogonal\n",
    "transformation) into the first few dimensions.\n",
    "For dimensionality reduction, only the top $l$ components are used. \n",
    "\n",
    "$\\ T_L = X_LW_L with T_LϵR^{nxl}$ and $\\ W_LϵR^{pxl} $\n",
    "\n",
    "if $l=2$ PCA finds the two-dimensional plane through the highdimensional dataset in which the data is most spread out so that if the data contains clusters these too may be most spread out and therefore most visible to be plotted.\n",
    "for PCA to be effective the data elements should be related to each other and it performs poor in the case of uncorrelated data.\n",
    "Since dimensionality reduction supposes that the data are distributed near a low dimensional manifold, one might choose PCA if the manifold is (approximately) linear, and nonlinear dimensionality reduction (NLDR) if the manifold is nonlinear.\n",
    "\n",
    "**UMAP (choose the other one)**  \n",
    "\n",
    "UMAP on the other hand was introduced to solve problems of dimensionality reduction algorithms using a algebraic and geometric approach. This algorithm is today considered the standard for dimensionality reduction in amny scientific application. The idea is to detect the manifold, geometrical structure where the points are laying, and trying to approximate this manifold somehow and projecting it in the target space.\n",
    "To implement this I start by assigning an open cover to each data point. if data lie in a metric space the open cover can be obtained by the balls centered in each data point thus there is a 0-simplex for each data point. But what is the optimal radius for each open cover?\n",
    "To choose this I need to stretch the space and adapt the notion of distance on the manifold so that all the points seem to be uniformly distributed infact if we bend the space, increasing dimensionality, we are guaranteed point to be uniformly disstributed as we want. Therefore we give to each point its own unique distance function, and select balls of radius one with respect to that local distance function.\n",
    "This translates to the unit ball about a point stretching to the k-th nearest neighbor of the point, where k is the sample size we are using to approximate the local sense of distance. Given the local metric, we can even weight the edges of the graph\n",
    "according to the edge vertices distance.\n",
    "Now we need now to faithfully embed the graph into a low-dim euclidean space so to preserve the original manifold structure.\n",
    "This means deciding which $\\ f(w_h,w_l)$ to optimize, where $\\ w_h,w_l$ are the graph\n",
    "edges weights in high and low dimension.\n",
    "The correct function is the cross-entropy.\n",
    "\n",
    "To see the differences between PCA and UMAP we can check their performances on the fashion mnist dataset. PCA performance is way worst. This is because PCA focuses on high variations. Infact PCA targets dimensionality, preserving large pairwise distances in the map, but cannot catch the structure of the data. On the contrary UMAP focuses on neighbourus rather than high variations and it is capable of reconstructing the structure of the data performing a very clean class separation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e93887",
   "metadata": {},
   "source": [
    "## t-SNE vs UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ea9fb",
   "metadata": {},
   "source": [
    "tSNE and UMAP are non-linear dimensionality reduction algorithms.\n",
    "Since PCA targets dimensionality, preserving large pairwise distances in the map, but cannot catch the structure of the data, the need of new algorithms brought to the rise of tSNE first and UMAP later.\n",
    "\n",
    "**tSNE**\n",
    "\n",
    "\n",
    "In tSNE what is reliable are the very small euclidean distances between neighbouring points in fact tSNE stands for           t-distributed stochastic neighbor embedding.\n",
    "This works by defining a gaussian distance on the original space and a t-student based distance on the lower dimensional space.\n",
    "tSNE infact uses joint instead of conditional probabilities. \n",
    "The optimisation function is:\n",
    "\n",
    "$$\\ C = KL(P∥Q) = \\sum \\limits _{i=1} ^{n} \\sum \\limits _{j=1} ^{n} p_{ij} log \\frac{p_{ij}}{q_{ij}}$$\n",
    "\n",
    "That is called: kullback-leibler divergence\n",
    "\n",
    "This introduces symmetry in the problem formulation, and the cost\n",
    "function optimisation is computationally much simpler. It also uses t-student distribution instead of gaussian modelling on the\n",
    "low-dimensional space so that this heavy-tailed distribution in the low-dimensional space alleviates both the crowding problem and the optimization problem by correcting volume differences between higher-dimensional and lower-dimensional spaces.\n",
    "In fact, it allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the\n",
    "map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar data points. This is due to the fact that the map’s representation of joint probabilities is (almost) invariant to changes in the scale of the map for map points that are far apart.\n",
    "The optimisation process relays on gradient descent.\n",
    "Every algorithm has a driving number to be set to modify its behaviour. this is the Hyperparameter perplexity in tTSNE\n",
    "This is the only only Hyperparameter to be set in tSNSE.\n",
    "Perplexity is the variance $\\ σ_i$ for the high-dim gaussian of $\\ p_{ij}$ .\n",
    "No single value of $\\ σ_i$ can be optimal for all data points in the data set because the density of the data is likely to vary. In dense regions, a smaller value of $\\ σ_i$ is usually more appropriate than in sparser regions.\n",
    "Any particular value of $\\ σ_i$ induces a probability distribution $\\ P_i$ that has an entropy which increases as $\\ σ_i$ increases. tSNE performs a binary search for the value of $\\ σ_i$ that produces a $\\ P_i$ with a fixed perplexity that is specified by the user:\n",
    "\n",
    "$$\\ Perp(P_i) = 2^{H(P_i)} = 2^{\\sum \\limits _{j} p_{ij} log_2 p_{ij}}$$\n",
    "\n",
    "Perplexity can be therefore interpreted as a smooth measure of the effective number of neighbors, typical values are between 5 and 50.\n",
    "\n",
    "The drawbacks of tSNE are that it does not perform output transformation, infact tSNE works on a given dataset and it's not giving you a map projecting the whole original space into the target dataset. It is only mapping the specific point of the dataset, it is stochastic and the output space is hard to interpret. Also the choice of the perplexity hyperparameter is critical and views at different perplexities are needed to understand the topology. In the end artifacts in data may appear, cluster size and inter-cluster distances are not meaningful and tSNE is slow on large datasets.\n",
    "\n",
    "**UMAP**\n",
    "\n",
    "UMAP on the other hand was introduced to solve problems of dimensionality reduction algorithms using a algebraic and geometric approach. This algorithm is today considered the standard for dimensionality reduction in amny scientific application. The idea is to detect the manifold, geometrical structure where the points are laying, and trying to approximate this manifold somehow and projecting it in the target space.\n",
    "To implement this I start by assigning an open cover to each data point. if data lie in a metric space the open cover can be obtained by the balls centered in each data point thus there is a 0-simplex for each data point. But what is the optimal radius for each open cover?\n",
    "To choose this I need to stretch the space and adapt the notion of distance on the manifold so that all the points seem to be uniformly distributed. Infact if we bend the space, increasing dimensionality, we are guaranteed point to be uniformly distributed as we want. Therefore we give to each point its own unique distance function, and select balls of radius one with respect to that local distance function.\n",
    "This translates to the unit ball about a point stretching to the k-th nearest neighbor of the point, where k is the sample size we are using to approximate the local sense of distance. Given the local metric, we can even weight the edges of the graph\n",
    "according to the edge vertices distance.\n",
    "Now we need now to faithfully embed the graph into a low-dim euclidean space so to preserve the original manifold structure.\n",
    "This means deciding which $\\ f(w_h,w_l)$ to optimize, where $\\ w_h,w_l$ are the graph\n",
    "edges weights in high and low dimension. In the case of UMAP, we are not optimizing the distribution of distances, just like we did with tSNE, but we optimize the relationship between the starting graph and the arrivng graph.\n",
    "The correct function is the cross-entropy.\n",
    "The Hyperparameters to set are: n_neighbours, min_dist and metric.\n",
    "The frist one refers to the number of neighboring points used in local approximations of manifold structure. Larger values will result in more global structure being preserved at the loss of detailed local structure.\n",
    "The second one controls how tightly the embedding is allowed to compress points together.\n",
    "The third one determines the choice of metric used to measure distance in the input space.\n",
    "\n",
    "In the contrary of tSNE, the function constructed by UMAP is also a real trasformation map and it can transform, not only the points in the dataset, but all the space where these points lay is mapped to a new space. I can than use a subset of my points to build the space and use it as test set. UMAP has also lower implementation time and greater performances.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be67dc9f",
   "metadata": {},
   "source": [
    "## PCA Vs MDS + differences between Classical, Metric and Non-Metric MDS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9ad2843",
   "metadata": {},
   "source": [
    "Multidimensional scaling is a family of algorithms visualizing the level of similarity of individuale cases of a dataset. In practice, mds finds an embedding of n objects into a r-dimensional euclidean space $R^n$ so to preserve as well as possible (a function of ) the distances between original objects. MDS can be distinguished in 3 kinds, depending on the objectivve function: \n",
    "* classical mds - the objective function is called strain and involves directly the original distances between objects. \n",
    "\n",
    "\n",
    "* metric mds - the objective function is called stress and involves a function of the original distances. \n",
    "\n",
    "Given a dimension $p$ and a monotone function $f$, metric mds aims at finding $X = (x_{i}, ..., x_{n})$ such that $||x_i - x_j|| = f(d_{ij})$ as close as possible, i.e minimising a chosen loss function (stress)\n",
    "\n",
    "* non-metric mds - the original distances are dissimilarities in the item-item matrix and the Euclidean distances between items, and defines the location of each of them. \n",
    "\n",
    "When dissimilarities are known only by their rank order, and the spacing between succesively ranked dissimilarities is of no interest or is unavailable. Here $f$ is only implicitely defines as a regression curve, and only preserves the order of d, that is $f(d_{ij} < f(d_{kl})$ if $d_{ij} < d_{kl}$ thus only the order of d is needed, not the actual values. The most common algorithm is the Kruskal mds.\n",
    "\n",
    "Examples: \n",
    "* classical mds - Eickman's colors: 14 colors, different only in hue (wavelength), pairwaise rated as similar in a 0-5 scale by 31 people. In this case we use the original distances which are provided by the 'similaity matrix'.\n",
    "* metric mds - Sammon mapping: The method was proposed by John W. Sammon in 1969. It is considered a non-linear approach as the mapping cannot be represented as a linear combination of the original variables as possible in techniques such as principal component analysis, which also makes it more difficult to use for classification applications. Denote the distance between ith and jth objects in the original space by ${\\displaystyle \\scriptstyle d_{ij}^{*}}\\scriptstyle d^{*}_{ij}$, and the distance between their projections by ${\\displaystyle \\scriptstyle d_{ij}^{}}\\scriptstyle d^{}_{ij}$. Sammon's mapping aims to minimize the following error function, which is often referred to as Sammon's stress or Sammon's error:$E = \\frac{1}{\\sum\\limits_{i<j}d^{*}_{ij}}\\sum_{i<j}\\frac{(d^{*}_{ij}-d_{ij})^2}{d^{*}_{ij}}.$  Sammon mapping preserves the small $d_{ij}$, giving them a greater degree of importance in the fitting procedure than for larger values of $d_{ij}$\n",
    "* non-metric mds -  An example of non-metric mds occurs each time we have only dissimilarities as rank order and the spacing between succesively ranked dissimilarities is of no interest or unavailable. Thus we could use non-metric mds when we have data about dissimilarity rating between sports.  \n",
    "\n",
    "\n",
    "Compared to PCA we can say that both the algorithms perform dimensionality reduction. Nevertheless, PCA tends to perform better in cases of linear dimensionality reduction, so when data are low dimensional and they require at max two-three dimension. On the other hand, MDS algorithm can be used also for  non-linear dimensionality reduction. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e3374ce",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfe093a",
   "metadata": {},
   "source": [
    "Many ML problems involve thousands of features, having such a large number of features bring along many problems, the most important ones are:\n",
    "* We cannot visualize data when we have more than 3 dimensions;\n",
    "* Makes the training extremely slow;\n",
    "* Makes it difficult to find a good solution\n",
    "This is known as the __curse of dimensionality__. The dimensionality reduction is the process of reducing the number of features to the most relevant ones. \n",
    "Namely, we want to reduce the dimensionality of our data retianing as much variance as possible. \n",
    "\n",
    "__MAIN APPROACHES FOR DIMENSIONALITY REDUCTION__\n",
    "The two main approaches to reducing dimensionality: _Projection_ and _Manifold Learning_:\n",
    "* _Projection_: this technique deals with projecting every data point which is in high dimension, onto a subspace suitable lower-dimensional space in a way which approximately preserves the distances between the points.\n",
    "* _Manifold Learning_ : many dimensionality reductions algorithm work by modelling the manifold on which the training instance lie; this is called Manifold learning. It relies on the manifold hypothesis or assumption, which holds that most real-world high-dimensional datasets lie close to a much lower-dimensional manifold, this assumption in most of the cases is based on observation or experience rather than theory or pure logic.\n",
    "\n",
    "__PCA (Principal Component Analysis)__\n",
    "PCA is an unsupervised algorithm used for dimensionality reduction; It works by identifying the hyperplane which lies closest to the data and then projects the data on that hyperplane while retaining most of the variation in the data set.\n",
    "The axis that explains the maximum amount of variance in the training set is called the _Principal Components_. The axis orthogonal to this axis is called the second principal component. As we go for higher dimensions, PCA would find a third component orthogonal to the other two components and so on, for visualization purposes we always stick to 2 or maximum 3 principal components.\n",
    "__t-SNE (T-distributed stochastic neighbour embedding)__\n",
    "t-SNE is particularly well suited for the visualization of high dimensional datasets. \n",
    "(t-SNE) takes a high dimensional data set and reduces it to a low dimensional graph that retains a lot of the original information. It does so by giving each data point a location in a two or three-dimensional map. This technique finds clusters in data thereby making sure that an embedding preserves the meaning in the data. t-SNE reduces dimensionality while trying to keep similar instances close and dissimilar instances apart.\n",
    "__UMAP (Uniform Manifold Approximation and Projection)__\n",
    "UMAP is a general-purpose manifold learning and dimension reduction algorithm. \n",
    "* UMAP is a nonlinear dimensionality reduction method, it is very effective for visualizing clusters or groups of data points and their relative proximities.\n",
    "The significant _difference with TSNE_ is scalability, it can be applied directly to sparse matrices thereby eliminating the need to applying any Dimensionality reduction such as PCA or Truncated SVD(Singular Value Decomposition) as a prior pre-processing step.\n",
    "\n",
    "* __PCA DRAWBACK__: he main drawback of PCA is that it is highly influenced by outliers present in the data. PCA is a linear projection, which means it can’t capture non-linear dependencies, its goal is to find the directions (the so-called principal components) that maximize the variance in a dataset.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba168194",
   "metadata": {},
   "source": [
    "## When PCA outperforms t-SNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b63bdf21",
   "metadata": {},
   "source": [
    "1) Stochasticity of final solution. PCA is deterministic; t-SNE is not. One gets a nice visualisation and then her colleague gets another visualisation and then they get artistic which looks better and if a difference of 0.03% in the $KL(P||Q)$ divergence is meaningful... In PCA the correct answer to the question posed is guaranteed. t-SNE might have multiple minima that might lead to different solutions. This necessitates multiple runs as well as raises questions about the reproducibility of the results.\n",
    "\n",
    "2) Interpretability of mapping. This relates to the above point but let's assume that a team has agreed in a particular random seed/run. Now the question becomes what this shows... t-SNE tries to map only local / neighbours correctly so our insights from that embedding should be very cautious; global trends are not accurately represented (and that can be potentially a great thing for visualisation). On the other hand, PCA is just a diagonal rotation of our initial covariance matrix and the eigenvectors represent a new axial system in the space spanned by our original data. We can directly explain what a particular PCA does.\n",
    "\n",
    "3) Application to new/unseen data. t-SNE is not learning a function from the original space to the new (lower) dimensional one and that's a problem. On that matter, t-SNE is a non-parametric learning algorithm so approximating with parametric algorithm is an ill-posed problem. The embedding is learned by directly moving the data across the low dimensional space. That means one does not get an eigenvector or a similar construct to use in new data. In contrast, using PCA the eigenvectors offer a new axes system what can be directly used to project new data. [Apparently one could try training a deep-network to learn the t-SNE mapping (you can hear Dr. van der Maaten at ~46' of this video suggesting something along this lines) but clearly no easy solution exists.]\n",
    "\n",
    "4) Incomplete data. Natively t-SNE does not deal with incomplete data. In fairness, PCA does not deal with them either but numerous extensions of PCA for incomplete data (eg. probabilistic PCA) are out there and are almost standard modelling routines. t-SNE currently cannot handle incomplete data (aside obviously training a probabilistic PCA first and passing the PC scores to t-SNE as inputs).\n",
    "\n",
    "5) The k is not (too) small case. t-SNE solves a problem known as the crowding problem, effectively that somewhat similar points in higher dimension collapsing on top of each other in lower dimensions (more here). Now as you increase the dimensions used the crowding problem gets less severe ie. the problem you are trying to solve through the use of t-SNE gets attenuated. You can work around this issue but it is not trivial. Therefore if you need a k dimensional vector as the reduced set and k is not quite small the optimality of the produce solution is in question. PCA on the other hand offer always the k best linear combination in terms of variance explained."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "341.333px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
